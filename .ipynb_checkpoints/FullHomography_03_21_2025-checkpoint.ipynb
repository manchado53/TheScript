{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key Point Prediction: Grid Search\n",
    "\n",
    "**March 19, 2025**\n",
    "\n",
    "# 1 Introduction\n",
    "\n",
    "In this notebook, we look to iterate on the original notebook for predicting key points and want to see if we can improve results using a grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in /home/keanej/.local/lib/python3.10/site-packages (2.6.0)\n",
      "Requirement already satisfied: filelock in /home/keanej/.local/lib/python3.10/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/keanej/.local/lib/python3.10/site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/keanej/.local/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/keanej/.local/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/keanej/.local/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/keanej/.local/lib/python3.10/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/keanej/.local/lib/python3.10/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/keanej/.local/lib/python3.10/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/keanej/.local/lib/python3.10/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/keanej/.local/lib/python3.10/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/keanej/.local/lib/python3.10/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /home/keanej/.local/lib/python3.10/site-packages (from torch) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/keanej/.local/lib/python3.10/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/keanej/.local/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/keanej/.local/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /home/keanej/.local/lib/python3.10/site-packages (from torch) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/keanej/.local/lib/python3.10/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/keanej/.local/lib/python3.10/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n"
     ]
    }
   ],
   "source": [
    "# !pip install YOLO\n",
    "# !pip install deprecation\n",
    "# !pip install albumentations\n",
    "# !pip install keras-tuner\n",
    "# !pip install ultralytics\n",
    "# !pip install --upgrade torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name '_functionalize_sync' from 'torch._utils' (/home/keanej/.local/lib/python3.10/site-packages/torch/_utils.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msoccer_util_last\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdistance_speed_stimator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01multralytics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m YOLO\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgrouping_teams\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstimating_team_position\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ultralytics/__init__.py:11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOMP_NUM_THREADS\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m      9\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOMP_NUM_THREADS\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# default for reduced CPU utilization during training\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01multralytics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NAS, RTDETR, SAM, YOLO, YOLOE, FastSAM, YOLOWorld\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01multralytics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ASSETS, SETTINGS\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01multralytics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchecks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_yolo \u001b[38;5;28;01mas\u001b[39;00m checks\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ultralytics/models/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Ultralytics ðŸš€ AGPL-3.0 License - https://ultralytics.com/license\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfastsam\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FastSAM\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnas\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NAS\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrtdetr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RTDETR\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ultralytics/models/fastsam/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Ultralytics ðŸš€ AGPL-3.0 License - https://ultralytics.com/license\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FastSAM\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpredict\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FastSAMPredictor\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mval\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FastSAMValidator\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ultralytics/models/fastsam/model.py:5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Ultralytics ðŸš€ AGPL-3.0 License - https://ultralytics.com/license\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpathlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01multralytics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Model\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpredict\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FastSAMPredictor\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mval\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FastSAMValidator\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ultralytics/engine/model.py:8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, Dict, List, Union\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01multralytics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcfg\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TASK2DATA, get_cfg, get_save_dir\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/__init__.py:52\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_running_with_deploy\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m builtins\u001b[38;5;241m.\u001b[39mbool:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mmodules\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch._meta_registrations\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mobject\u001b[39m\n\u001b[0;32m---> 52\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     53\u001b[0m     _functionalize_sync \u001b[38;5;28;01mas\u001b[39;00m _sync,\n\u001b[1;32m     54\u001b[0m     _import_dotted_name,\n\u001b[1;32m     55\u001b[0m     classproperty,\n\u001b[1;32m     56\u001b[0m )\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_utils_internal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     58\u001b[0m     get_file_path,\n\u001b[1;32m     59\u001b[0m     prepare_multiprocessing_environment,\n\u001b[1;32m     60\u001b[0m     USE_GLOBAL_DEPS,\n\u001b[1;32m     61\u001b[0m     USE_RTLD_GLOBAL_WITH_LIBTORCH,\n\u001b[1;32m     62\u001b[0m )\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# TODO(torch_deploy) figure out how to freeze version.py in fbcode build\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name '_functionalize_sync' from 'torch._utils' (/home/keanej/.local/lib/python3.10/site-packages/torch/_utils.py)"
     ]
    }
   ],
   "source": [
    "from soccer_util_last import *\n",
    "from distance_speed_stimator import *\n",
    "from ultralytics import YOLO\n",
    "from grouping_teams import *\n",
    "from stimating_team_position import *\n",
    "from heatmap import *\n",
    "from possesion_tracker import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Data\n",
    "\n",
    "Here, we load the data and show how we can map the labeled key points to their respective images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = get_dataset()\n",
    "print(dataset.shape)\n",
    "print(dataset.columns)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[['file', 'x','y','label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"'{dataset.label.iloc[1]}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:   \n",
    "    visualize_point_layout(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_image =  cv2.imread(f\"{PATH}/images/frame_8.jpg\")\n",
    "first_image_height, first_image_width = first_image.shape[:2]\n",
    "first_image_df = dataset[dataset['file'] == 'frame_8.jpg']\n",
    "\n",
    "for _, row in first_image_df.iterrows():\n",
    "        x, y = row[\"x\"], row[\"y\"]\n",
    "        label = row[\"label\"]\n",
    "        \n",
    "        # Draw the point on the image (circle)\n",
    "        x_full, y_full = int(x * first_image_width + 5), int(y * first_image_height + 5)\n",
    "        cv2.circle(first_image, \n",
    "                   (x_full, y_full), \n",
    "                   radius=5, color=(0, 0, 255), thickness=-1)  # Red circle\n",
    "\n",
    "        # Add label near the point\n",
    "        cv2.putText(first_image, label, (x_full + 10, y_full - 10), cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                    fontScale=1, color=(255, 255, 255), thickness=2, lineType=cv2.LINE_AA)  # White text\n",
    "\n",
    "image_rgb = cv2.cvtColor(first_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Display the image using Matplotlib\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(image_rgb)\n",
    "plt.axis('off')  # Turn off axis\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Flip Augmentations\n",
    "\n",
    "This has to be done somewhat delicately to not corrupt the model, but we should be able to flip the image and the notions of left/right to double our dataset and normalize a little for some dataset bias if there are more samples from one angle over others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(f\"{PATH}/images/augmentations\"):\n",
    "    os.mkdir(f\"{PATH}/images/augmentations\")\n",
    "\n",
    "augmented_sets = []\n",
    "\n",
    "for file in dataset.file.unique():\n",
    "    if \"augmentations\" in file:\n",
    "        continue\n",
    "\n",
    "    points = dataset[dataset.file == file].copy()\n",
    "    img = cv2.imread(f\"{PATH}/images/{file}\")\n",
    "    img = cv2.flip(img, 1)\n",
    "    augmented_file = f\"augmentations/flip_{file}\"\n",
    "    cv2.imwrite(f\"{PATH}/images/{augmented_file}\", img)\n",
    "\n",
    "    points_flipped = points.copy()\n",
    "    points_flipped[\"x\"] = 1 - points[\"x\"]\n",
    "    points_flipped[\"label\"] = points_flipped[\"label\"].str.replace(\"Left\", \"tmp\").str.replace(\"Right\", \"Left\").str.replace(\"tmp\", \"Right\")\n",
    "    points_flipped[\"label_frame_index\"] += dataset.shape[0]\n",
    "    points_flipped = points_flipped.sort_values(by=[\"label_frame_index\", \"label\"]).reset_index(drop=True)\n",
    "    points_flipped[\"file\"] = augmented_file\n",
    "    augmented_sets.append(points_flipped)\n",
    "\n",
    "augmented_dataset = pd.concat((dataset, *augmented_sets)).reset_index(drop=True)\n",
    "augmented_dataset.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we create train-test split with 20% of the data being used for the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    X_train,\n",
    "    X_val,\n",
    "    y_visibility_train,\n",
    "    y_visibility_val,\n",
    "    y_coordinates_train,\n",
    "    y_coordinates_val,\n",
    ") = get_train_test_split(augmented_dataset, test_size=0.20)\n",
    "\n",
    "USE_NEGATIVE_1_MASKING = True\n",
    "\n",
    "# for this notebook, I use (-1, -1) as the location for a missing point because of the custom loss function used\n",
    "if USE_NEGATIVE_1_MASKING:\n",
    "    y_coordinates_train = y_coordinates_train.copy()\n",
    "    for i in range(y_coordinates_train.shape[0]):\n",
    "        y_coordinates_train[i, y_visibility_train[i] == 0, :] = -1\n",
    "\n",
    "    y_coordinates_val = y_coordinates_val.copy()\n",
    "    for i in range(y_coordinates_val.shape[0]):\n",
    "        y_coordinates_val[i, y_visibility_val[i] == 0, :] = -1\n",
    "\n",
    "\n",
    "print(\"X_train:\", X_train.shape,\n",
    "      \"\\nX_val:\", X_val.shape,\n",
    "      \"\\ny_visibility_train:\", y_visibility_train.shape,\n",
    "      \"\\ny_visibility_val:\", y_visibility_val.shape,\n",
    "      \"\\ny_coordinates_train:\", y_coordinates_train.shape,\n",
    "      \"\\ny_coordinates_val:\", y_coordinates_val.shape\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    visualize_samples(X_train, y_coordinates_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our model, we want to see if better configurations can be used to improve on our validation masked masked MAE loss because this will be key for getting our coordinate prediction correct. For initial hyperparamters to experiments with, we use the following in a hyperparameter grid search:\n",
    "\n",
    "- learning rate\n",
    "\n",
    "- number of dense neurons coming out of convolutional layers\n",
    "\n",
    "- number of dense neurons in the layer used for further extracting features for coordinate prediction\n",
    "\n",
    "- amount of dropout to be used during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_builder(hp, input_shape=(TARGET_SIZE[1], TARGET_SIZE[0], 3)):\n",
    "    # --- Defining Hyperparameters ---\n",
    "    hp_lr = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4, 1e-5, 1e-6])\n",
    "    hp_decay_rate = hp.Float('decay_rate', min_value=0.95, max_value=1.0)\n",
    "    hp_dense_neurons = hp.Int('hp_dense_neurons', min_value=32, max_value=512, step=32)\n",
    "    hp_coord_dense_neurons = hp.Int('hp_coord_dense_neurons', min_value=32, max_value=512, step=32)\n",
    "    hp_dropout_rate = hp.Choice('dropout', values=[0.1, 0.2, 0.3, 0.4])\n",
    "\n",
    "    # --- Model Definition ---\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    # -- Convolutional Layers --\n",
    "    x = Conv2D(32, (3, 3), activation=\"relu\", kernel_initializer=\"he_normal\")(inputs)\n",
    "    x = MaxPooling2D(pool_size=(4, 4))(x)\n",
    "    x = Dropout(hp_dropout_rate)(x)  # Added dropout\n",
    "\n",
    "    x = Conv2D(64, (3, 3), activation=\"relu\", kernel_initializer=\"he_normal\")(x)\n",
    "    x = MaxPooling2D(pool_size=(4, 4))(x)\n",
    "    x = Dropout(hp_dropout_rate)(x)  # Added dropout\n",
    "\n",
    "    x = Conv2D(128, (3, 3), activation=\"relu\", kernel_initializer=\"he_normal\")(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Dropout(hp_dropout_rate)(x)  # Added dropout\n",
    "\n",
    "    # -- Dense Layers --\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(hp_dense_neurons, activation=\"relu\", kernel_initializer=\"he_normal\")(x)\n",
    "    x = Dropout(hp_dropout_rate)(x)\n",
    "\n",
    "    coord_x = Dense(hp_coord_dense_neurons, activation=\"relu\", kernel_initializer=\"he_normal\")(x)\n",
    "\n",
    "    # -- Outputs --\n",
    "    # Coordinates Output (12 keypoints, each with (x, y), linear activation)\n",
    "    coordinates_output = Dense(12 * 2, activation=\"linear\", kernel_initializer=\"he_normal\")(coord_x)\n",
    "    coordinates_output = Reshape((12, 2), name=\"xy\")(coordinates_output)\n",
    "\n",
    "    # Visibility Output (12 units, sigmoid activation)\n",
    "    visibility_output = Dense(12, activation=\"sigmoid\", name=\"visible\")(x)\n",
    "\n",
    "    # Define the model with two outputs\n",
    "    model = Model(inputs=inputs, outputs=[visibility_output, coordinates_output])\n",
    "\n",
    "    # --- Optimizer ---\n",
    "    initial_lr = hp_lr\n",
    "    decay_rate = hp_decay_rate\n",
    "\n",
    "    # Define the ExponentialDecay schedule\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=initial_lr,\n",
    "        decay_steps=1,  # The number of steps per epoch (here it decays every epoch)\n",
    "        decay_rate=decay_rate,\n",
    "        staircase=False  # Whether to apply the decay in a staircase fashion\n",
    "    )\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "    # Compile the model with a combined loss function\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss={\"visible\": \"binary_crossentropy\", \"xy\": MaskedMAELoss()},\n",
    "        loss_weights={\"visible\": 1, \"xy\": 10},\n",
    "        metrics={\"visible\": RoundedAccuracy(), \"xy\": \"mae\"},\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define our model tuner using a Hyperband search in accordance with the tuner defined in this tutorial (https://www.tensorflow.org/tutorials/keras/keras_tuner)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = 'hp_search_v6'\n",
    "tuner = kt.BayesianOptimization(\n",
    "    model_builder,\n",
    "    objective=kt.Objective(\"val_xy_loss\", direction=\"min\"),\n",
    "    max_trials=50,\n",
    "    directory='model_dir',\n",
    "    project_name=project_name\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"loss\",  # Metric to monitor (use 'val_loss' or 'val_accuracy', etc.)\n",
    "    patience=10,  # Number of epochs with no improvement after which training will be stopped\n",
    "    restore_best_weights=True,  # Whether to restore model weights from the epoch with the best value of the monitored quantity\n",
    "    mode=\"min\",  # 'min' for minimizing the monitored quantity, 'max' for maximizing\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    tuner.search(\n",
    "        X_train,\n",
    "        [y_visibility_train, y_coordinates_train],\n",
    "        epochs=200,\n",
    "        batch_size=4,\n",
    "        validation_data=(X_val, [y_visibility_val, y_coordinates_val]),\n",
    "        callbacks=[early_stopping],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We look at the best model hyperparameters found from the search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_hps = tuner.get_best_hyperparameters()[0]\n",
    "# print(json.dumps(best_hps.values, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we refit this model so we can get a look at results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIT_MODEL = False\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "if FIT_MODEL:\n",
    "    model = tuner.hypermodel.build(best_hps)\n",
    "    history = model.fit(\n",
    "        X_train,\n",
    "        [y_visibility_train, y_coordinates_train],\n",
    "        epochs=200,\n",
    "        batch_size=4,\n",
    "        validation_data=(X_val, [y_visibility_val, y_coordinates_val]),\n",
    "        callbacks=[early_stopping],\n",
    "    )\n",
    "    model.save(f\"/data/ai_club/SoccerStats2024/keypoint_model_v0_03_20_2025_{project_name}.keras\")\n",
    "else:\n",
    "    model =keras.models.load_model(\n",
    "        f\"/data/ai_club/SoccerStats2024/keypoint_model_v0_03_20_2025_{project_name}.keras\",\n",
    "        custom_objects={\n",
    "            \"MaskedMAELoss\": MaskedMAELoss(),\n",
    "            \"RoundedAccuracy\": RoundedAccuracy(),\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FIT_MODEL:\n",
    "    plot_training_run(history)\n",
    "else:\n",
    "    print(\"Model was loaded in from previous run.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.sysconfig.get_build_info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.version.cuda)\n",
    "from matplotlib.lines import Line2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_TRAINING_DATA = False\n",
    "INDEX = 2\n",
    "SHOW_AVERAGE_POINTS = False\n",
    "if USE_TRAINING_DATA:\n",
    "    evaluate_prediction(\n",
    "        augmented_dataset,\n",
    "        model,\n",
    "        MaskedMAELoss(),\n",
    "        X_train,\n",
    "        y_visibility_train,\n",
    "        y_coordinates_train,\n",
    "        index=INDEX,\n",
    "        show_average_points=SHOW_AVERAGE_POINTS,\n",
    "    )\n",
    "else:\n",
    "    evaluate_prediction(\n",
    "        augmented_dataset,\n",
    "        model,\n",
    "        MaskedMAELoss(),\n",
    "        X_val,\n",
    "        y_visibility_val,\n",
    "        y_coordinates_val,\n",
    "        index=INDEX,\n",
    "        show_average_points=SHOW_AVERAGE_POINTS,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_train = evaluate_model_metrics(model, X_train, y_visibility_train, y_coordinates_train)\n",
    "metrics_validation = evaluate_model_metrics(model, X_val, y_visibility_val, y_coordinates_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Homography"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img_path = f\"{PATH}/images/28f4a31a-frame_707.jpg\"\n",
    "img = cv2.cvtColor(cv2.imread(test_img_path), cv2.COLOR_BGR2RGB)\n",
    "img_preproccsed, _ = load_and_preprocess_image_and_coords_lines(test_img_path, enhance_lines=True, resize=True)\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(12, 6))\n",
    "if False:\n",
    "\n",
    "    ax[0].imshow(img)\n",
    "    ax[0].set_title(\"Base Image\")\n",
    "\n",
    "    ax[1].imshow(img_preproccsed)\n",
    "    ax[1].set_title(\"Preprocessed Image\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_vis, y_pred_xy = model.predict(np.array([img_preproccsed]))\n",
    "image = visualize_keypoints(image=img_preproccsed, keypoints=y_pred_xy.squeeze()[y_pred_vis.squeeze().round().astype(int) == 1])\n",
    "plt.imshow(image)\n",
    "plt.title(\"Keypoint Predictions\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Example dimensions\n",
    "TARGET_SIZE = (710, 400)  # width, height\n",
    "X_combined = np.concatenate([np.array(X_train), np.array(X_val)], axis=0)\n",
    "y_vis_combined = np.concatenate([np.array(y_visibility_train), np.array(y_visibility_val)], axis=0)\n",
    "y_coords_combined = np.concatenate([np.array(y_coordinates_train), np.array(y_coordinates_val)], axis=0)\n",
    "mae = compute_homography_mae_over_dataset(\n",
    "    X=X_combined,\n",
    "    y_pred_vis=y_vis_combined,\n",
    "    y_pred_xy=y_coords_combined,\n",
    "    dest_keypoints=dest_keypoints,\n",
    "    target_size=TARGET_SIZE,\n",
    "    affine = False\n",
    ")\n",
    "\n",
    "print(mae)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_pts, H = calculate_H(y_pred_xy.squeeze() , y_pred_vis.squeeze())\n",
    "plot_in_2D(transformed_pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred_vis, y_pred_xy = model.predict(X_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = compute_homography_mae_over_dataset(\n",
    "    X=X_combined,\n",
    "    y_pred_vis=y_pred_vis,\n",
    "    y_pred_xy=y_pred_xy,\n",
    "    dest_keypoints=dest_keypoints,\n",
    "    target_size=TARGET_SIZE,\n",
    "    affine = False\n",
    ")\n",
    "print(mae)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_vis, y_pred_xy = model.predict(X_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "projection_error = compute_projection_error_over_dataset(\n",
    "    X_combined,\n",
    "    y_pred_vis,\n",
    "    y_pred_xy,\n",
    "    y_vis_combined,\n",
    "    y_coords_combined\n",
    ")\n",
    "print(projection_error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(visualize_keypoints(\n",
    "    image=X_val[0],\n",
    "    keypoints=y_coordinates_val[0],\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset():\n",
    "    labels_data = json.load(open(f\"{PATH}/{LABELS_JSON}\", \"r\"))\n",
    "\n",
    "    for sample in labels_data:\n",
    "        assert len(sample[\"annotations\"]) == 1\n",
    "\n",
    "    keypoint_labels = []\n",
    "\n",
    "    for i, sample in enumerate(labels_data):\n",
    "        keypoint_labels += get_sample_information(sample, i)\n",
    "\n",
    "    keypoint_labels_df = pd.DataFrame(keypoint_labels)\n",
    "    transformed_pts, H = calculate_H(y_coordinates_val[3], y_visibility_val[3])\n",
    "\n",
    "    \n",
    "    all_labels = keypoint_labels_df.label.value_counts().keys().tolist()\n",
    "\n",
    "    # Apply function to each frame\n",
    "    df_filled = keypoint_labels_df.groupby(\"file\", group_keys=False).apply(\n",
    "        lambda group: fill_missing_labels(group, all_labels)\n",
    "    )\n",
    "\n",
    "    # Sort to maintain consistent label order within each frame\n",
    "    df_filled = df_filled.sort_values(by=[\"label_frame_index\", \"label\"]).reset_index(\n",
    "        drop=True\n",
    "    )\n",
    "\n",
    "    return df_filled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Keypoints_in_2D = {\n",
    "    \"Left Bottom Corner\": (0, WIDTH_FIELD),\n",
    "    \"Right Bottom Corner\": (LENGTH_FIELD, WIDTH_FIELD),\n",
    "    \"Left Top Corner\": (0, 0),\n",
    "    \"Right Top Corner\": (LENGTH_FIELD, 0),\n",
    "    \"Left Top Box Corner\": (LENGTH_BOX, (WIDTH_FIELD / 2)- (WIDTH_BOX/2)),\n",
    "    \"Left Bottom  Box Corner\": (LENGTH_BOX, (WIDTH_FIELD / 2)+ (WIDTH_BOX/2)),\n",
    "    \"Box Top Right  Corner\": (LENGTH_FIELD - LENGTH_BOX, (WIDTH_FIELD / 2)- (WIDTH_BOX/2)),\n",
    "    \"Box Bottom Right Corner\": (LENGTH_FIELD - LENGTH_BOX, (WIDTH_FIELD / 2)+ (WIDTH_BOX/2)),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_keypoints(keypoints):\n",
    "    \"\"\"\n",
    "    Scale the keypoints to the target size.\n",
    "    \"\"\"\n",
    "    scale_x, scale_y = get_scale_factors()\n",
    "    scaled_keypoints = []\n",
    "    for keypoint in keypoints:\n",
    "        x = keypoint[0] * scale_x\n",
    "        y = keypoint[1] * scale_y\n",
    "        scaled_keypoints.append((x, y))\n",
    "    return np.array(scaled_keypoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_keypoints = scale_keypoints(Keypoints_in_2D.values())\n",
    "scaled_keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_homography(points_2d, H):\n",
    "    # Convert to homogeneous coordinates: (N, 2) â†’ (N, 3)\n",
    "    H_inv = np.linalg.inv(H)\n",
    "\n",
    "    # Prepare for perspectiveTransform\n",
    "    points = np.array(points_2d, dtype=np.float32).reshape(-1, 1, 2)\n",
    "\n",
    "    # Apply inverse homography\n",
    "    sideview_points = cv2.perspectiveTransform(points, H_inv)\n",
    "    sideview_points = sideview_points.reshape(-1, 2)\n",
    "        # Normalize: divide by width and height\n",
    "    norm_x = sideview_points[:, 0] / TARGET_SIZE[0]\n",
    "    norm_y = sideview_points[:, 1] / TARGET_SIZE[1]\n",
    "\n",
    "    normalized = np.stack([norm_x, norm_y], axis=1)\n",
    "    for index, x, y in enumerate(normalized):\n",
    "        if x < 0 or x > 1 or y < 0 or y > 1:\n",
    "            normalized[index][0] = -1\n",
    "            normalized[index][1] = -1\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_homography(scaled_keypoints, H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_pts, H = calculate_H(y_coordinates_val[3], y_visibility_val[3])\n",
    "\n",
    "plt.imshow(visualize_keypoints(\n",
    "    image=X_val[3],\n",
    "    keypoints=apply_homography(scaled_keypoints, H),\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Player Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "yolo_model = YOLO(\"yolov8x.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(f\"{PATH}/images/f19e8eb9-frame_427.jpg\")\n",
    "img = cv2.cvtColor(first_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "results = yolo_model(img)\n",
    "annotated_img = results[0].plot()\n",
    "\n",
    "# Display with matplotlib\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(annotated_img)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"YOLOv8 Detection Results\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we can also apply homography to these points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottom_points = []\n",
    "\n",
    "for box in results[0].boxes:\n",
    "    x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()  # (x1, y1, x2, y2)\n",
    "\n",
    "    # normalize by the image being inferenced on\n",
    "    x1 /= annotated_img.shape[1]\n",
    "    y1 /= annotated_img.shape[0]\n",
    "    x2 /= annotated_img.shape[1]\n",
    "    y2 /= annotated_img.shape[0]\n",
    "    \n",
    "    # Bottom-center of bounding box\n",
    "    bottom_x = (x1 + x2) / 2\n",
    "    bottom_y = y2\n",
    "    \n",
    "    bottom_points.append([bottom_x, bottom_y])\n",
    "    \n",
    "bottom_points_np = np.array(bottom_points, dtype=np.float32)\n",
    "normalized_pts = bottom_points_np.copy()\n",
    "\n",
    "transformed_bottom_pts = transform_points(normalized_pts, H)\n",
    "plot_in_2D(transformed_bottom_pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_pts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 Team Recognition\n",
    "\n",
    "We can use KMeans based on pixel values to help us make predictions of what teams are where on the field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box_color_map = []\n",
    "image_w_boxes= results[0].plot()\n",
    "for box in results[0].boxes:\n",
    "    x1, y1, x2, y2 = box.xyxy[0].cpu().numpy().astype(int)\n",
    "    \n",
    "    # Middle pixel\n",
    "    mid_x = int((x1 + x2) / 2)\n",
    "    mid_y = int((y1 + y2) / 2)\n",
    "\n",
    "    # Sample BGR color at middle pixel\n",
    "    color = results[0].plot()[mid_y, mid_x]  # shape (3,) â†’ [B, G, R]\n",
    "    box_color_map.append({\n",
    "        \"box\": (x1, y1, x2, y2),\n",
    "        \"color\": color\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_box_colors(results, img, patch_size=5):\n",
    "    \"\"\"Extract average color from the center patch of each detected box.\"\"\"\n",
    "    box_infos = []\n",
    "    colors = []\n",
    "\n",
    "    for box in results:\n",
    "        x1, y1, x2, y2 = box\n",
    "\n",
    "        mid_x = int((x1 + x2) / 2)\n",
    "        mid_y = int((y1 + y2) / 2)\n",
    "\n",
    "        half_patch = patch_size // 2\n",
    "        patch = img[max(0, mid_y - half_patch):mid_y + half_patch + 1,\n",
    "                    max(0, mid_x - half_patch):mid_x + half_patch + 1]\n",
    "\n",
    "        avg_color = patch.mean(axis=(0, 1)) if patch.size > 0 else np.array([0, 0, 0])\n",
    "        box_infos.append(((x1, y1, x2, y2), avg_color))\n",
    "        colors.append(avg_color)\n",
    "\n",
    "    return box_infos, np.array(colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_boxes_by_color_distance(box_infos, threshold=100):\n",
    "    \"\"\"Group boxes manually based on color distance threshold.\"\"\"\n",
    "    groups = []\n",
    "\n",
    "    for box, color in box_infos:\n",
    "        assigned = False\n",
    "        for group in groups:\n",
    "            dist = np.linalg.norm(color - group[\"color\"])\n",
    "            if dist < threshold:\n",
    "                group[\"boxes\"].append(box)\n",
    "                assigned = True\n",
    "                break\n",
    "        if not assigned:\n",
    "            groups.append({\n",
    "                \"color\": color,\n",
    "                \"boxes\": [box]\n",
    "            })\n",
    "\n",
    "    return groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "colors = np.array([entry[\"color\"] for entry in box_color_map])\n",
    "kmeans = KMeans(n_clusters=2).fit(colors)\n",
    "\n",
    "# Add cluster labels\n",
    "for idx, entry in enumerate(box_color_map):\n",
    "    entry[\"cluster\"] = int(kmeans.labels_[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_boxes_by_kmeans(box_infos, colors, n_clusters=2):\n",
    "    \"\"\"Cluster box colors using KMeans.\"\"\"\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(colors)\n",
    "    clustered_boxes = {i: [] for i in range(n_clusters)}\n",
    "\n",
    "    for i, (box, color) in enumerate(box_infos):\n",
    "        cluster = kmeans.labels_[i]\n",
    "        clustered_boxes[cluster].append(box)\n",
    "\n",
    "    return clustered_boxes, kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_boxes_by_kmeans(box_infos, colors, n_clusters=2):\n",
    "    \"\"\"Cluster box colors using KMeans and make sure white is always cluster 0.\"\"\"\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(colors)\n",
    "\n",
    "    # Step 1: Get cluster centers and find the one closest to white (255, 255, 255)\n",
    "    white = np.array([255, 255, 255])\n",
    "    distances = np.linalg.norm(kmeans.cluster_centers_ - white, axis=1)\n",
    "    white_cluster_idx = np.argmin(distances)  # Closest to white\n",
    "\n",
    "    # Step 2: Remap cluster labels so white is always cluster 0\n",
    "    label_mapping = {white_cluster_idx: 0}\n",
    "    next_label = 1\n",
    "    for i in range(n_clusters):\n",
    "        if i != white_cluster_idx:\n",
    "            label_mapping[i] = next_label\n",
    "            next_label += 1\n",
    "\n",
    "    # Step 3: Apply remapping to boxes\n",
    "    clustered_boxes = {i: [] for i in range(n_clusters)}\n",
    "    for i, (box, color) in enumerate(box_infos):\n",
    "        original_cluster = kmeans.labels_[i]\n",
    "        mapped_cluster = label_mapping[original_cluster]\n",
    "        clustered_boxes[mapped_cluster].append(box)\n",
    "\n",
    "    return clustered_boxes, kmeans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_teams(results, img, n_clusters=2, patch_size=3):\n",
    "    \"\"\"Full pipeline: extract box colors, cluster using KMeans.\"\"\"\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    box_infos, colors = extract_box_colors(results, img)\n",
    "    clustered_boxes, kmeans = cluster_boxes_by_kmeans(box_infos, colors, n_clusters)\n",
    "    return clustered_boxes, kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "clustered_boxes, kmeans_model = calculate_teams(results, img, n_clusters=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors_for_clusters = [(255, 0, 0), (0, 255, 0), (0, 0, 255)]  # Add more if needed\n",
    "\n",
    "for cluster_id, boxes in clustered_boxes.items():\n",
    "    color = colors_for_clusters[cluster_id % len(colors_for_clusters)]\n",
    "    for (x1, y1, x2, y2) in boxes:\n",
    "        cv2.rectangle(img, (x1, y1), (x2, y2), color, 2)\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "box_infos = []\n",
    "colors = []\n",
    "\n",
    "for box in results[0].boxes:\n",
    "    x1, y1, x2, y2 = box.xyxy[0].cpu().numpy().astype(int)\n",
    "\n",
    "    # Middle of the box\n",
    "    mid_x = int((x1 + x2) / 2)\n",
    "    mid_y = int((y1 + y2) / 2)\n",
    "\n",
    "    # Extract a small patch (5x5) around the center to avoid noise\n",
    "    patch = img[max(0, mid_y-2):mid_y+3, max(0, mid_x-2):mid_x+3]\n",
    "\n",
    "    # Compute average color of patch\n",
    "    avg_color = patch.mean(axis=(0, 1))  # BGR\n",
    "\n",
    "    box_infos.append(((x1, y1, x2, y2), avg_color))\n",
    "    colors.append(avg_color)\n",
    "\n",
    "colors = np.array(colors)\n",
    "\n",
    "# Run KMeans on the colors\n",
    "k = 2  # or 3 if you expect 3 groups (e.g., 2 teams + ref)\n",
    "kmeans = KMeans(n_clusters=k, random_state=0).fit(colors)\n",
    "\n",
    "# Assign cluster labels\n",
    "clustered_boxes = {i: [] for i in range(k)}\n",
    "for i, (box, color) in enumerate(box_infos):\n",
    "    cluster = kmeans.labels_[i]\n",
    "    clustered_boxes[cluster].append(box)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy original image\n",
    "vis_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Assign random color per cluster\n",
    "# cluster_colors = [tuple(np.random.randint(0, 255, 3).tolist()) for _ in range(k)]\n",
    "cluster_colors = [\n",
    "    (255, 0, 0),\n",
    "    (0, 0, 255),\n",
    "]\n",
    "\n",
    "for cluster_id, boxes in clustered_boxes.items():\n",
    "    color = cluster_colors[cluster_id]\n",
    "    for x1, y1, x2, y2 in boxes:\n",
    "        cv2.rectangle(vis_img, (x1, y1), (x2, y2), color, 2)\n",
    "        cv2.putText(\n",
    "            vis_img,\n",
    "            f\"Team {cluster_id+1}\",\n",
    "            (x1, y1 - 5),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            0.5,\n",
    "            color,\n",
    "            1,\n",
    "        )\n",
    "\n",
    "# Convert to RGB for display with matplotlib\n",
    "vis_img_rgb = cv2.cvtColor(vis_img, cv2.COLOR_BGR2RGB)\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(vis_img_rgb)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"YOLO Boxes Grouped by Jersey Color\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can put this all together using the homography projection!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.markers as mmarkers\n",
    "\n",
    "field_img = cv2.imread(viets_field_img)\n",
    "field_img = cv2.cvtColor(field_img, cv2.COLOR_BGR2RGB)\n",
    "cluster_labels = kmeans.labels_\n",
    "\n",
    "# Display the transformed image\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(field_img)\n",
    "plt.scatter(\n",
    "    [int(x) for x, _ in transformed_pts],\n",
    "    [int(y) for _, y in transformed_pts],\n",
    "    c=\"white\",\n",
    "    s=8,\n",
    "    label=\"Keypoint\",\n",
    ")\n",
    "\n",
    "# Team 1\n",
    "plt.scatter(\n",
    "    transformed_bottom_pts[cluster_labels == 0, 0],\n",
    "    transformed_bottom_pts[cluster_labels == 0, 1],\n",
    "    c=\"blue\",\n",
    "    s=42,\n",
    "    label=\"Opponent\",\n",
    "    marker='x'\n",
    ")\n",
    "\n",
    "# Team 2\n",
    "plt.scatter(\n",
    "    transformed_bottom_pts[cluster_labels == 1, 0],\n",
    "    transformed_bottom_pts[cluster_labels == 1, 1],\n",
    "    c=\"red\",\n",
    "    s=42,\n",
    "    label=\"MSOE\",\n",
    "    marker=mmarkers.MarkerStyle('o', fillstyle='none')\n",
    ")\n",
    "\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Soccer Player Homography\")\n",
    "legend = plt.legend(\n",
    "    bbox_to_anchor=(1.01, 1),\n",
    "    loc=\"upper left\",\n",
    ")\n",
    "frame = legend.get_frame()\n",
    "frame.set_facecolor(\"lightgray\")\n",
    "plt.xlim(0, field_img.shape[1])\n",
    "plt.ylim(field_img.shape[0], 0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "players_grouped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Applying all steps into a video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recognising with yolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# video_path = '/data/ai_club/SoccerStats2024/30SecondsAurora.mp4'\n",
    "# output_path = '/data/ai_club/SoccerStats2024/AuroraVideo2D.mp4'\n",
    "# cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "# viets_field_img_obj = cv2.imread(viets_field_img)\n",
    "# viets_field_img_height, viets_field_img_width = viets_field_img_obj.shape[:2]\n",
    "\n",
    "\n",
    "# field_small_width = 300  # or any size you want\n",
    "# field_small_height = int(viets_field_img_height * (field_small_width / viets_field_img_width))\n",
    "# viets_field_small = cv2.resize(viets_field_img_obj, (field_small_width, field_small_height))\n",
    "# field_alpha = 0.5  # transparency level\n",
    "\n",
    "# fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "# out = cv2.VideoWriter(output_path, fourcc, fps, (viets_field_img_width, viets_field_img_height))\n",
    "\n",
    "# H_smooth = None\n",
    "# alpha = 0.8  \n",
    "# last_positions = None\n",
    "\n",
    "# frame_idx = 0\n",
    "# while cap.isOpened() and frame_idx <300:\n",
    "#     new_map = viets_field_img_obj.copy()\n",
    "#     ret, frame = cap.read()\n",
    "#     if not ret:\n",
    "#         print(\"Brokem\")\n",
    "#         break\n",
    "        \n",
    "\n",
    "#     if frame_idx % 3 == 0:\n",
    "#         # === Your Pipeline ===\n",
    "#         results_video = yolo_model(frame)[0].boxes.xyxy\n",
    "#         players_pos = extract_normalized_bottom_centers(results_video, frame.shape[:2])\n",
    "#         players_grouped, _ = calculate_teams(results, img, n_clusters=2)\n",
    "#         frame, coord = load_and_preprocess_image_and_coords_lines(frame)\n",
    "#         frame_input = np.expand_dims(frame, axis=0)  # Add batch dimension\n",
    "\n",
    "#         y_pred_vis, y_pred_xy = model.predict(frame_input)\n",
    "#         y_pred_vis = y_pred_vis.squeeze().round().astype(int)\n",
    "\n",
    "\n",
    "\n",
    "#         if sum(y_pred_vis) >= 4:\n",
    "#             _, H = calculate_H(y_pred_xy.squeeze(), y_pred_vis.squeeze())\n",
    "#             H_smooth = alpha * H_smooth + (1 - alpha) * H if H_smooth is not None else H\n",
    "\n",
    "#             player_2d_positions = transform_points(players_pos, H_smooth)\n",
    "#             last_positions = player_2d_positions\n",
    "#             # === Optional: Draw on frame ===\n",
    "# #             for pt in player_2d_positions:\n",
    "# #                 x, y = int(pt[0]), int(pt[1] )  # scale if needed\n",
    "# #                 cv2.circle(new_map, (x, y), 5, (0, 255, 0), -1)\n",
    "#             for group_id, boxes in players_grouped.items():\n",
    "#                 color = group_colors.get(group_id, (0, 255, 0))  # Default green if undefined\n",
    "#                 for box in boxes:\n",
    "#                     pt = box_to_2d.get(tuple(box))\n",
    "#                     if pt is not None:\n",
    "#                         x, y = int(pt[0]), int(pt[1])\n",
    "#                         cv2.circle(new_map, (x, y), 5, color, -1)\n",
    "#     elif last_positions is not None:\n",
    "#         for pt in last_positions:\n",
    "#             x, y = int(pt[0]), int(pt[1])\n",
    "#             cv2.circle(new_map, (x, y), 5, (0, 255, 0), -1)\n",
    "#     cv2.putText(new_map, f\"Frame {frame_idx}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "#                 1, (255, 255, 255), 2)\n",
    "#     out.write(new_map)\n",
    "#     frame_idx += 1\n",
    "\n",
    "# cap.release()\n",
    "# out.release()\n",
    "# print(\"Processing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the video_segments dictionary from the file\n",
    "with open(\"/data/ai_club/SoccerStats2024/adrian_video.pkl\", 'rb') as f:\n",
    "    bboxes = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/data/ai_club/SoccerStats2024/ball_220frames_filled_smooth.pkl\", 'rb') as f:\n",
    "    ball_positions = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "video_path = '/data/ai_club/SoccerStats2024/30SecondsAurora.mp4'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "player_color_sum = defaultdict(lambda: np.zeros(3))\n",
    "player_color_count = defaultdict(int)\n",
    "\n",
    "frame_idx = 0\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret or frame_idx >= len(bboxes):\n",
    "        \n",
    "        break\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    player_boxes = bboxes[frame_idx]  # {id: [(x1, y1, x2, y2)]}\n",
    "\n",
    "    # Build list of just box coords for your function\n",
    "    flat_boxes = []\n",
    "    id_lookup = []\n",
    "\n",
    "    for player_id, boxes in player_boxes.items():\n",
    "        for box in boxes:\n",
    "            flat_boxes.append(box)\n",
    "            id_lookup.append(player_id)\n",
    "\n",
    "    # Use your function to extract chest-patch colors\n",
    "    box_infos, _ = extract_box_colors(flat_boxes, frame)\n",
    "\n",
    "    for i, (box, avg_color) in enumerate(box_infos):\n",
    "        player_id = id_lookup[i]\n",
    "        player_color_sum[player_id] += avg_color\n",
    "        player_color_count[player_id] += 1\n",
    "\n",
    "    frame_idx += 1\n",
    "\n",
    "cap.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_avg_color = {}\n",
    "\n",
    "for player_id in player_color_sum:\n",
    "    player_avg_color[player_id] = (\n",
    "        player_color_sum[player_id] / player_color_count[player_id]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_ids = list(player_avg_color.keys())\n",
    "color_values = np.array([player_avg_color[pid] for pid in player_ids])  # shape: (N, 3)\n",
    "\n",
    "# Run KMeans\n",
    "kmeans = KMeans(n_clusters=3, random_state=0).fit(color_values)\n",
    "labels = kmeans.labels_  # labels[i] corresponds to player_ids[i]\n",
    "\n",
    "# Map player_id â†’ cluster (0 or 1)\n",
    "player_id_to_group = {\n",
    "    player_ids[i]: int(labels[i])\n",
    "    for i in range(len(player_ids))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_avg_color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_id_to_group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating map from video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define paths for input video and output video\n",
    "# video_path = '/data/ai_club/SoccerStats2024/30SecondsAurora.mp4'\n",
    "# output_path = '/data/ai_club/SoccerStats2024/AuroraVideo2D.mp4'\n",
    "\n",
    "# # Open the video file\n",
    "# cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# # Get the frames per second (fps) of the video\n",
    "# fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "# print(fps)\n",
    "\n",
    "# # Load the field image and get its dimensions\n",
    "# viets_field_img_obj = cv2.imread(viets_field_img)\n",
    "# viets_field_img_height, viets_field_img_width = viets_field_img_obj.shape[:2]\n",
    "\n",
    "# # Resize the field image for overlay purposes\n",
    "# field_small_width = 300  # Desired width for the resized field image\n",
    "# field_small_height = int(viets_field_img_height * (field_small_width / viets_field_img_width))\n",
    "# viets_field_small = cv2.resize(viets_field_img_obj, (field_small_width, field_small_height))\n",
    "# field_alpha = 0.5  # Transparency level for overlay\n",
    "\n",
    "# # Initialize the video writer for the output video\n",
    "# fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "# out = cv2.VideoWriter(output_path, fourcc, fps, (viets_field_img_width, viets_field_img_height))\n",
    "\n",
    "# # Initialize variables for smoothing and tracking\n",
    "# H_smooth = None\n",
    "# alpha = 0.8  # Smoothing factor for homography matrix\n",
    "# frame_idx = 0\n",
    "# last_group_positions = {}  # Dictionary to store last known positions of players\n",
    "\n",
    "# # Process each frame of the video\n",
    "# while cap.isOpened():\n",
    "#     # Create a copy of the field image for drawing\n",
    "#     new_map = viets_field_img_obj.copy()\n",
    "    \n",
    "#     # Read the next frame from the video\n",
    "#     ret, frame = cap.read()\n",
    "#     if not ret:\n",
    "#         print(\"Brokem\")  # Break the loop if no frame is read\n",
    "#         break\n",
    "    \n",
    "#     # Convert the frame to RGB format\n",
    "#     frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "#     # Process every second frame\n",
    "#     if frame_idx % 2 == 0:\n",
    "#         # Get bounding boxes for players in the current frame\n",
    "#         player_boxes_frame = bboxes[frame_idx]  # {player_id: [(x1, y1, x2, y2)]}\n",
    "\n",
    "#         # Flatten bounding boxes and track player IDs\n",
    "#         flat_boxes = []\n",
    "#         player_id_list = []\n",
    "#         for player_id, boxes in player_boxes_frame.items():\n",
    "#             for box in boxes:\n",
    "#                 flat_boxes.append(box)\n",
    "#                 player_id_list.append(player_id)\n",
    "\n",
    "#         # Extract bottom-center points of bounding boxes\n",
    "#         player_positions = extract_normalized_bottom_centers(flat_boxes, frame.shape[:2])\n",
    "\n",
    "#         # Preprocess the frame for keypoint detection\n",
    "#         frame_preprocessed, coord = load_and_preprocess_image_and_coords_lines(frame)\n",
    "#         frame_input = np.expand_dims(frame_preprocessed, axis=0)\n",
    "\n",
    "#         # Predict visibility and keypoints using the model\n",
    "#         y_pred_vis, y_pred_xy = model.predict(frame_input)\n",
    "#         y_pred_vis = y_pred_vis.squeeze().round().astype(int)\n",
    "\n",
    "#         # If sufficient keypoints are visible, calculate homography\n",
    "#         if sum(y_pred_vis) >= 4:\n",
    "#             _, H = calculate_H(y_pred_xy.squeeze(), y_pred_vis.squeeze())\n",
    "#             H_smooth = alpha * H_smooth + (1 - alpha) * H if H_smooth is not None else H\n",
    "\n",
    "#             # Transform player positions to 2D field coordinates\n",
    "#             player_2d_positions = transform_points(player_positions, H_smooth)\n",
    "\n",
    "#             # Update last known positions of players\n",
    "#             last_group_positions = {}\n",
    "#             for idx, player_id in enumerate(player_id_list):\n",
    "#                 pt = player_2d_positions[idx]\n",
    "#                 if player_id not in last_group_positions:\n",
    "#                     last_group_positions[player_id] = []\n",
    "#                 last_group_positions[player_id].append(pt)\n",
    "\n",
    "#             # Draw players on the field using their group colors\n",
    "#             for player_id, points in last_group_positions.items():\n",
    "#                 group = player_id_to_group.get(player_id)  # Get group ID for the player\n",
    "#                 color = group_colors.get(group)  # Get color for the group\n",
    "#                 for pt in points:\n",
    "#                     x, y = int(pt[0]), int(pt[1])\n",
    "#                     cv2.circle(new_map, (x, y), 14, color, -1)\n",
    "\n",
    "#     # If not processing the current frame, use the last known positions\n",
    "#     elif last_group_positions:\n",
    "#         for player_id, points in last_group_positions.items():\n",
    "#             group = player_id_to_group.get(player_id)\n",
    "#             color = group_colors.get(group)\n",
    "#             for pt in points:\n",
    "#                 x, y = int(pt[0]), int(pt[1])\n",
    "#                 cv2.circle(new_map, (x, y), 14, color, -1)\n",
    "\n",
    "#     # Write the processed frame to the output video\n",
    "#     out.write(new_map)\n",
    "#     frame_idx += 1\n",
    "\n",
    "# # Release resources\n",
    "# cap.release()\n",
    "# out.release()\n",
    "# print(\"Processing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import distance_speed_stimator\n",
    "importlib.reload(distance_speed_stimator)\n",
    "\n",
    "from distance_speed_stimator import *\n",
    "importlib.reload(heatmap)\n",
    "import stimating_team_position\n",
    "importlib.reload(stimating_team_position)\n",
    "from stimating_team_position import update_zone_tracking\n",
    "import possesion_tracker\n",
    "importlib.reload(possesion_tracker)\n",
    "from possesion_tracker import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths for input video and output video\n",
    "video_path = '/data/ai_club/SoccerStats2024/30SecondsAurora.mp4'\n",
    "output_path_3D = '/data/ai_club/SoccerStats2024/AuroraVideo3D.mp4'\n",
    "tracker = PossessionTracker(3, 3)\n",
    "\n",
    "# Open the video file\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Get video properties\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "print(f\"FPS: {fps}, Resolution: {frame_width}x{frame_height}\")\n",
    "\n",
    "# Load the field image and get its dimensions\n",
    "viets_field_img_obj = cv2.imread(viets_field_img)\n",
    "viets_field_img_height, viets_field_img_width = viets_field_img_obj.shape[:2]\n",
    "\n",
    "# 3D VideoWriter (draw speed directly on video frame)\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out3D = cv2.VideoWriter(output_path_3D, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "# Tracking variables\n",
    "H_smooth = None\n",
    "alpha = 0.8\n",
    "frame_idx = 0\n",
    "last_group_positions = {}  # player_id -> (pt_2d, frame_pt, group, color, speed)\n",
    "reset_player_history()\n",
    "# Process frames\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Video read error or finished.\")\n",
    "        break\n",
    "    frame_orig = frame.copy()\n",
    "    # frame_orig = cv2.resize(frame_orig, (TARGET_SIZE[0], TARGET_SIZE[1]))\n",
    "    frame_orig = cv2.cvtColor(frame_orig, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    if frame_idx % 5 == 0:\n",
    "        frame_preprocessed, coord = load_and_preprocess_image_and_coords_lines(frame_orig)\n",
    "        frame_input = np.expand_dims(frame_preprocessed, axis=0)\n",
    "        y_pred_vis, y_pred_xy = model.predict(frame_input)\n",
    "        y_pred_vis = y_pred_vis.squeeze().round().astype(int)\n",
    "\n",
    "        if sum(y_pred_vis) >= 4:\n",
    "            _, H = calculate_H(y_pred_xy.squeeze(), y_pred_vis.squeeze())\n",
    "            H_smooth = alpha * H_smooth + (1 - alpha) * H if H_smooth is not None else H\n",
    "    if frame_idx % 2 == 0 and H_smooth is not None:\n",
    "        player_boxes_frame = bboxes.get(frame_idx, {})  # {player_id: [(x1, y1, x2, y2)]}\n",
    "        \n",
    "        flat_boxes = []\n",
    "        player_id_list = []\n",
    "        for player_id, boxes in player_boxes_frame.items():\n",
    "            for box in boxes:\n",
    "                flat_boxes.append(box)\n",
    "                player_id_list.append(player_id)\n",
    "        # Extract bottom-center positions of players\n",
    "        player_positions = extract_normalized_bottom_centers(flat_boxes, frame_orig.shape[:2])  # image-space points\n",
    "\n",
    "\n",
    "        if sum(y_pred_vis) >= 4:\n",
    "            \n",
    "            player_2d_positions = transform_points(player_positions, H_smooth)\n",
    "            draw_data = []\n",
    "            for idx, player_id in enumerate(player_id_list):\n",
    "                pt_2d = player_2d_positions[idx]\n",
    "                frame_pt = player_positions[idx]  # original frame position\n",
    "                group = player_id_to_group.get(player_id)\n",
    "\n",
    "                update_player_history(player_id, pt_2d, frame_idx, group)\n",
    "                speed, last_distance = calculate_speed_distance_mps(player_id, fps)\n",
    "                total_distance = update_player_stats(player_id, speed, last_distance)\n",
    "\n",
    "                color = group_colors.get(group)\n",
    "                draw_data.append((player_id, pt_2d, frame_pt, group, color, speed, total_distance))\n",
    "            detect_off_ball_run(10, 0, fps)\n",
    "\n",
    "            # Store data for next frame\n",
    "            last_group_positions = {\n",
    "                player_id: (pt_2d, frame_pt, group, color, speed, total_distance)\n",
    "                for player_id, pt_2d, frame_pt, group, color, speed, total_distance in draw_data\n",
    "            }\n",
    "            # update_zone_tracking(last_group_positions)\n",
    "            player_positions = {\n",
    "                player_id: pt_2d\n",
    "                for player_id, pt_2d, _, _, _, _, _ in draw_data\n",
    "            }\n",
    "            # \n",
    "            ## LOGIC FOR BALL\n",
    "            ball_box = ball_positions[frame_idx]\n",
    "            if len(ball_box) == 4:\n",
    "                ball_position_normalized = extract_normalized_bottom_centers([ball_box], frame_orig.shape[:2])\n",
    "                ball_2d_position = transform_points(ball_position_normalized, H_smooth)\n",
    "                last_ball_position = ball_2d_position[0]\n",
    "            \n",
    "            tracker.update_possession(last_ball_position, player_positions,  player_id_to_group)\n",
    "\n",
    "    # Draw current or previous positions with speed on 3D frame\n",
    "    for player_id, (pt_2d, frame_pt, group, color, speed, total_distance) in last_group_positions.items():\n",
    "        frame_x, frame_y = int(frame_pt[0] * frame_width), int(frame_pt[1] * frame_height)\n",
    "        speed_text = f\"{speed:.1f} m/s\"\n",
    "        total_distance_text = f\"Dist: {total_distance:.1f} m\"\n",
    "        cv2.circle(frame_orig, (frame_x, frame_y), 8, color, -1)\n",
    "        cv2.putText(frame_orig, speed_text, (frame_x + 10, frame_y - 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "        cv2.putText(frame_orig, total_distance_text, (frame_x + 10, frame_y + 20),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "\n",
    "    ## Draw ball position\n",
    "    ball_x, ball_y = int(ball_box[0] * frame_width), int(ball_box[1] * frame_height)\n",
    "    cv2.circle(frame_orig, (ball_x, ball_y), 8, (0, 255, 0), -1)\n",
    "    cv2.putText(frame_orig, \"Ball\", (ball_x + 10, ball_y - 10),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "    # Draw total possession of ball\n",
    "    total_possession = tracker.get_total_possession()\n",
    "    team_0_possession = total_possession.get(0, 0)\n",
    "    team_1_possession = total_possession.get(1, 0)\n",
    "\n",
    "    # Format possession text\n",
    "    possession_text_0 = f\"Team 0: {team_0_possession:.1f}%\"\n",
    "    possession_text_1 = f\"Team 1: {team_1_possession:.1f}%\"\n",
    "\n",
    "    # Define the box dimensions for the top-right corner\n",
    "    box_width, box_height = 600, 70\n",
    "    box_x, box_y = frame_width - box_width - 20, 20  # 20px margin from the top-right corner\n",
    "\n",
    "    # Draw the box for Team 0\n",
    "    cv2.rectangle(frame_orig, (box_x, box_y), (box_x + box_width, box_y + box_height), (0, 0, 0), -1)\n",
    "    cv2.putText(frame_orig, possession_text_0, (box_x + 10, box_y + 45),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1.2, (255, 255, 255), 2)\n",
    "\n",
    "    # Draw the box for Team 1 below Team 0\n",
    "    box_y_below = box_y + box_height + 10  # Add 10px margin between boxes\n",
    "    cv2.rectangle(frame_orig, (box_x, box_y_below), (box_x + box_width, box_y_below + box_height), (0, 0, 0), -1)\n",
    "    cv2.putText(frame_orig, possession_text_1, (box_x + 10, box_y_below + 45),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1.2, (255, 255, 255), 2)\n",
    "\n",
    "    # Save frame\n",
    "    out3D.write(cv2.cvtColor(frame_orig, cv2.COLOR_RGB2BGR))\n",
    "    frame_idx += 1\n",
    "# heatmap = create_player_heatmap(player_id= 10)\n",
    "# Cleanup\n",
    "cap.release()\n",
    "out3D.release()\n",
    "print(\"Processing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_off_ball_runs(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_team_possession_heatmap(self, team_id, output_path=\"team_possession_heatmap.jpg\", alpha=0.6):\n",
    "    \"\"\"\n",
    "    Draws a heatmap of possession percentages for a given team using different tones of white with transparency.\n",
    "    \"\"\"\n",
    "    # Load the field image\n",
    "    field_image = cv2.imread(viets_field_img)\n",
    "    field_image = cv2.cvtColor(field_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Create a blank heatmap with the same dimensions as the field image\n",
    "    heatmap = np.zeros_like(field_image, dtype=np.uint8)\n",
    "\n",
    "    # Draw vertical and horizontal lines to separate zones\n",
    "    for col in range(1, self.vertical_zones):\n",
    "        x = col * self.zone_width\n",
    "        cv2.line(heatmap, (x, 0), (x, self.field_height), (255, 0, 0), 3)  # Blue lines\n",
    "    for row in range(1, self.horizontal_zones):\n",
    "        y = row * self.zone_height\n",
    "        cv2.line(heatmap, (0, y), (self.field_width, y), (255, 0, 0), 3)  # Blue lines\n",
    "\n",
    "    # Overlay possession percentages for the given team\n",
    "    possession_percentages = self.get_possession_percentages()\n",
    "    for row in range(self.horizontal_zones):\n",
    "        for col in range(self.vertical_zones):\n",
    "            percentages = possession_percentages.get((row, col), {})\n",
    "            team_percent = percentages.get(team_id, 0)\n",
    "\n",
    "            # Calculate the intensity of white based on possession percentage\n",
    "            intensity = int(255 * (team_percent / 100))\n",
    "            color = (intensity, intensity, intensity)  # Shades of white\n",
    "\n",
    "            # Fill the zone with the calculated color\n",
    "            top_left = (col * self.zone_width, row * self.zone_height)\n",
    "            bottom_right = ((col + 1) * self.zone_width, (row + 1) * self.zone_height)\n",
    "            cv2.rectangle(heatmap, top_left, bottom_right, color, -1)\n",
    "\n",
    "            # Add possession percentage text\n",
    "            x = int((col + 0.5) * self.zone_width)\n",
    "            y = int((row + 0.5) * self.zone_height)\n",
    "            cv2.putText(\n",
    "                heatmap,\n",
    "                f\"{team_percent:.1f}%\",\n",
    "                (x - 30, y),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                0.7,\n",
    "                (0, 0, 0),  # Black text for contrast\n",
    "                2,\n",
    "                cv2.LINE_AA,\n",
    "            )\n",
    "\n",
    "    # Blend the heatmap with the field image\n",
    "    blended_image = cv2.addWeighted(heatmap, alpha, field_image, 1 - alpha, 0)\n",
    "\n",
    "    # Save the heatmap image\n",
    "    cv2.imwrite(output_path, cv2.cvtColor(blended_image, cv2.COLOR_RGB2BGR))\n",
    "    print(f\"Team possession heatmap saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_team_possession_heatmap(tracker, team_id=0, output_path=\"team_0_possession_heatmap.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_distance_players(top_n=5):\n",
    "    \"\"\"Return the top N players with the highest distance covered.\"\"\"\n",
    "    player_stats = get_player_stats()\n",
    "    sorted_players = sorted(player_stats.items(), key=lambda item: item[1][1], reverse=True)\n",
    "    return sorted_players[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_top_distance_players(top_n=5):\n",
    "    \"\"\"Display a horizontal bar chart of the top N players with the highest distance covered.\"\"\"\n",
    "    top_players = get_top_distance_players(top_n)\n",
    "    player_ids = [str(player_id) for player_id, _ in top_players]  # Convert IDs to strings\n",
    "    distances = [distance for _, (_, distance) in top_players]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(player_ids, distances, color='skyblue')\n",
    "    plt.ylabel('Player ID')\n",
    "    plt.xlabel('Distance Covered (m)')\n",
    "    plt.title(f'Top {top_n} Players by Distance Covered')\n",
    "    plt.gca().invert_yaxis()  # Invert y-axis to show the highest distance at the top\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_top_distance_players(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_top_distance_players(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap  = create_player_heatmap(player_id= 10)     \n",
    "heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)     \n",
    "cv2.imwrite(\"test_heatmap.png\", heatmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = '/data/ai_club/SoccerStats2024/30SecondsAurora.mp4'\n",
    "frames = []\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    frames.append(frame)\n",
    "\n",
    "cap.release()\n",
    "frames = np.array(frames)\n",
    "image = frames[88]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "frame_preprocessed, coord = load_and_preprocess_image_and_coords_lines(image)\n",
    "frame_input = np.expand_dims(frame_preprocessed, axis=0)\n",
    "y_pred_vis, y_pred_xy = model.predict(frame_input)\n",
    "vis_mask = y_pred_vis.round().astype(int).flatten() == 1\n",
    "\n",
    "y_pred_xy = y_pred_xy[0][vis_mask]\n",
    "visible_labels = np.array(list(dest_keypoints.keys()))[vis_mask]\n",
    "\n",
    "image_after = visualize_keypoints(image=image, keypoints=y_pred_xy.squeeze(), labels =visible_labels, radius =18, font_scale=2, font_thickness=3)\n",
    "cv2.imwrite('/data/ai_club/SoccerStats2024/ImagesSlides/keypoints.jpg', cv2.cvtColor(image_after, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "plt.imshow(image_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_with_boxes = image.copy()\n",
    "\n",
    "for player_id, boxes in bboxes[88].items():\n",
    "    for box in boxes:\n",
    "        if len(box) == 4:\n",
    "            x1, y1, x2, y2 = box\n",
    "            cv2.rectangle(image_with_boxes, (x1, y1), (x2, y2), color=(0, 0, 255), thickness=4)\n",
    "            cv2.putText(image_with_boxes, str(player_id), (x1, y1 - 10),\n",
    "                        fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=0.7, color=(0, 0, 255), thickness=4)\n",
    "cv2.imwrite('/data/ai_club/SoccerStats2024/ImagesSlides/image_boxes.jpg', image_with_boxes)\n",
    "image_with_boxes = cv2.cvtColor(image_with_boxes, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "plt.imshow(image_with_boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_colors = [\n",
    "    (255, 255, 255),  # Cluster 0 â†’ white âœ…\n",
    "    (255, 0, 0),      # Cluster 1 â†’ dark blue (BGR) âœ…\n",
    "    (0, 255, 255),    # Cluster 2 â†’ bright yellow (BGR) âœ…\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_with_cluster_boxes = frames[310].copy()\n",
    "\n",
    "\n",
    "for player_id, box_list in bboxes[310].items():\n",
    "    cluster_id = player_id_to_group.get(player_id, 0)\n",
    "    color = cluster_colors[cluster_id]\n",
    "\n",
    "    for box in box_list:\n",
    "        if len(box) != 4:\n",
    "            continue  # skip invalid boxes\n",
    "\n",
    "        x1, y1, x2, y2 = box\n",
    "\n",
    "        # cv2.rectangle(image_with_cluster_boxes, (x1, y1), (x2, y2), color=color, thickness=4)\n",
    "        # cv2.putText(image_with_cluster_boxes, str(player_id), (x1, y1 - 10),\n",
    "        #             fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=0.7, color=color, thickness=4)\n",
    "        \n",
    "cv2.imwrite('/data/ai_club/SoccerStats2024/ImagesSlides/image_boxes_clusters_after.jpg',image_with_cluster_boxes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##how to reload python file\n",
    "import importlib\n",
    "import video_creator\n",
    "importlib.reload(video_creator)\n",
    "from video_creator import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_video_3D(frames, bboxes, player_id_to_group, ball_positions, output_path=\"/data/ai_club/SoccerStats2024/ImagesSlides/players_and_ball_video.mp4\", fps=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ima = cv2.imread(viets_field_img)\n",
    "print(ima.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_2D_map(frames, bboxes, player_id_to_group, ball_positions, output_path=\"/data/ai_club/SoccerStats2024/ImagesSlides/2d_map.mp4\",\n",
    "                              fps=30, model =model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import possesion_tracker\n",
    "importlib.reload(possesion_tracker)\n",
    "from possesion_tracker import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the possession tracker before the loop\n",
    "tracker = PossessionTracker(zone_count=3)\n",
    "H_smooth = None\n",
    "alpha = 0.8\n",
    "last_group_positions = {}  # player_id -> (pt_2d, frame_pt, group, color, speed, total_distance)\n",
    "\n",
    "# Start looping through frames\n",
    "for frame_idx, frame in enumerate(frames):\n",
    "    frame_orig = frame.copy()\n",
    "    frame_orig = cv2.cvtColor(frame_orig, cv2.COLOR_RGB2BGR)\n",
    "    if frame_idx % 5 == 0:\n",
    "        frame_preprocessed, coord = load_and_preprocess_image_and_coords_lines(frame_orig)\n",
    "        frame_input = np.expand_dims(frame_preprocessed, axis=0)\n",
    "        y_pred_vis, y_pred_xy = model.predict(frame_input)\n",
    "        y_pred_vis = y_pred_vis.squeeze().round().astype(int)\n",
    "\n",
    "        if sum(y_pred_vis) >= 4:\n",
    "            _, H = calculate_H(y_pred_xy.squeeze(), y_pred_vis.squeeze())\n",
    "            H_smooth = alpha * H_smooth + (1 - alpha) * H if H_smooth is not None else H\n",
    "\n",
    "    if frame_idx % 2 == 0 and H_smooth is not None:\n",
    "        player_boxes_frame = bboxes.get(frame_idx, {})  # {player_id: [(x1, y1, x2, y2)]}\n",
    "        \n",
    "        flat_boxes = []\n",
    "        player_id_list = []\n",
    "        for player_id, boxes in player_boxes_frame.items():\n",
    "            for box in boxes:\n",
    "                flat_boxes.append(box)\n",
    "                player_id_list.append(player_id)\n",
    "        # Extract bottom-center positions of players\n",
    "        player_positions = extract_normalized_bottom_centers(flat_boxes, frame_orig.shape[:2])  # image-space points\n",
    "\n",
    "\n",
    "        if sum(y_pred_vis) >= 4:\n",
    "            player_2d_positions = transform_points(player_positions, H_smooth)\n",
    "            draw_data = []\n",
    "            for idx, player_id in enumerate(player_id_list):\n",
    "                pt_2d = player_2d_positions[idx]\n",
    "                frame_pt = player_positions[idx]  # original frame position\n",
    "                group = player_id_to_group.get(player_id)\n",
    "\n",
    "                update_player_history(player_id, pt_2d, frame_idx, group)\n",
    "                # speed, last_distance = calculate_speed_distance_mps(player_id, fps)\n",
    "                # total_distance = update_player_stats(player_id, speed, last_distance)\n",
    "                # color = group_colors.get(group)\n",
    "                # draw_data.append((player_id, pt_2d, frame_pt, group, color, speed, total_distance))\n",
    "                last_group_positions[player_id] = tuple(pt_2d)\n",
    "\n",
    "            # update_zone_tracking(last_group_positions)\n",
    "\n",
    "                # ---- BALL POSITION TRACKING ----\n",
    "            last_ball_position = None  # Reset each frame\n",
    "\n",
    "            if frame_idx < len(ball_positions) and H_smooth is not None:\n",
    "                ball_box = ball_positions[frame_idx]\n",
    "                if len(ball_box) == 4:\n",
    "                    ball_position_normalized = extract_normalized_bottom_centers([ball_box], frame_orig.shape[:2])\n",
    "                    ball_2d_position = transform_points(ball_position_normalized, H_smooth)\n",
    "                    last_ball_position = ball_2d_position[0]\n",
    "\n",
    "\n",
    "    print(\"Frame Index:\", frame_idx)\n",
    "    # ---- POSSESSION UPDATE ----\n",
    "    tracker.update_possession(\n",
    "        ball_position=last_ball_position,\n",
    "        player_positions=last_group_positions,\n",
    "        player_id_to_group=player_id_to_group\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(frames[486])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker.get_possession_percentages()\n",
    "tracker.print_summary()\n",
    "tracker.draw_zones_with_possession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentages = get_zone_occupancy_percentages()\n",
    "percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import stimating_team_position\n",
    "importlib.reload(stimating_team_position)\n",
    "from stimating_team_position import draw_zone_occupancy_overlay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_percentages = draw_zone_occupancy_overlay(percentages)\n",
    "img_rgb = cv2.cvtColor(image_percentages, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Display it\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(img_rgb)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Zone Occupancy Overlay\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
